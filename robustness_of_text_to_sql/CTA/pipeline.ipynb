{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from copy import deepcopy\n",
    "import enchant\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from tqdm import trange, tqdm\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import inflect\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ebf43",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a044e-aa88-4457-897e-4ca542b967b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_reservered_words = ['against', 'and', 'area', 'average', 'since', 'away', 'section', 'by', 'class', 'club', 'code', 'cup', 'current', 'date', 'data', 'district', 'elected', 'engine', 'episode', 'event', 'final', 'finish', 'first', 'for', 'from', 'game', 'games', 'goals', 'gold', 'grid', 'height', 'high', 'home', 'id', 'in', 'incumbent', 'international', 'laps', 'league', 'list', 'log', 'loss', 'losses', 'lost', 'method', 'age', 'name', 'nation', 'no', 'notes', 'number', 'of', 'one', 'two', 'three', 'four', 'yes', 'no', 'yards', 'five' 'other', 'outcome', 'overall', 'par', 'party', 'per', 'pick', 'played', 'player', 'points', 'pos', 'rank', 'record', 'region', 'release', 'report', 'res', 'result', 'results','round', 'score', 'season', 'second', 'series', 'singles', 'start', 'end', 'state', 'status', 'table', 'team', 'types', 'the', 'first', 'second', 'third', 'time', 'to', 'total', 'type', 'up', 'week', 'weeks', 'year', 'unit', 'version', 'years', 'ends', 'ended', 'min', 'max', 'make', 'statistics', 'stats', 'in', 'on', 'to', 'see', 'feet', 'subject']\n",
    "preps = [\"aboard\",\"about\",\"above\",\"across\",\"after\",\"against\",\"along\",\"amid\",\"among\",\"as\",\"at\",\"before\",\"behind\",\"below\",\"beneath\",\"beside\",\"besides\",\"between\",\"beyond\",\"but\",\"by\",\"concerning\",\"considering\",\"despite\",\"down\",\"during\",\"except\",\"excepting\",\"excluding\",\"following\",\"for\",\"from\",\"in\",\"inside\",\"into\",\"like\",\"minus\",\"near\",\"of\",\"off\",\"on\",\"onto\",\"opposite\",\"outside\",\"over\",\"past\",\"per\",\"plus\",\"regarding\",\"round\",\"save\",\"since\",\"than\",\"through\",\"to\",\"toward\",\"towards\",\"under\",\"underneath\",\"unlike\",\"until\",\"up\",\"upon\",\"versus\",\"via\",\"with\",\"within\",\"without\"]\n",
    "bigram_reservered_words = list(set(bigram_reservered_words + preps))\n",
    "\n",
    "\n",
    "\n",
    "### Templates to be used for checking\n",
    "# template1 = lambda table_name, col: f\"We are told of the {table_name}'s {trim_col_name(table_name, col)}.\"\n",
    "# template2 = lambda table_name, col: f\"We are informed of the {trim_col_name(table_name, col)} of the {table_name}.\"\n",
    "# template3 = lambda table_name, col: f\"We know {trim_col_name(table_name, col)} of the {table_name}.\"\n",
    "# template4 = lambda table_name, col: f\"We collect {table_name}'s {trim_col_name(table_name, col)}.\"\n",
    "# TEMPLATES = [template1, template2, template3, template4] if STRICT_MODE else [template1, template2]\n",
    "tablename_black_list = [\"statistics\", \"data\", \"table\", \"summary\", \"sketch\", \"list\"]\n",
    "date_marks = [\"date\", \"dates\", \"year\", \"years\", \"month\", \"months\",\n",
    "              \"day\", \"days\", \"daytime\", \"minute\", \"minutes\", \"second\", \"seconds\", \"time\"]\n",
    "num_marks = [\"num\", \"number\", \"sum\", \"amount\", \"count\", \"total\", \"#\", \"No.\", \"no.\", \"scores\",\n",
    "             \"rating\", \"rank\", \"height\", \"weight\", \"age\", \"time\", \"times\", \"temperature\",\n",
    "             \"year\", \"years\", \"month\", \"months\", \"day\", \"days\", \"minute\", \"minutes\", \n",
    "             \"second\", \"seconds\", \"average\", \"sum\", \"grade\", \"fee\", \"cost\", \"value\",\n",
    "             \"rate\"]  # words explicitly has numeric implications\n",
    "\n",
    "\n",
    "\n",
    "def template1(table_name, col, col_type):\n",
    "    \"\"\"Template 1 placeholder filling for give table name, column name, and column type\"\"\"\n",
    "    table_name = get_singular_word(table_name)\n",
    "    capital_tname = table_name[0].capitalize() + table_name[1:]\n",
    "    trimmed_col_name = trim_col_naive(table_name, col)\n",
    "    type_prompt = trim_type(col_type)\n",
    "    if col_type == \"date\" and any([dm in trimmed_col_name for dm in date_marks]):\n",
    "        type_prompt = \"\"\n",
    "    if col_type == \"number\" and any([nm in trimmed_col_name for nm in num_marks]):\n",
    "        type_prompt = \"\"\n",
    "    return f\"{capital_tname} {trimmed_col_name}{trim_type(col_type)}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccc56f-4825-41ea-aad3-c70846cc23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATES = [template1]\n",
    "device = \"cuda\"\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f8069",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflect_engine = inflect.engine()\n",
    "def get_singular_word(word):\n",
    "    \"\"\"\n",
    "    Reduce a given word (string) to singular form\n",
    "    \"\"\"\n",
    "    ans = inflect_engine.singular_noun(word)\n",
    "    return ans if ans else word\n",
    "\n",
    "\n",
    "def read_dense_table_vectors(path, delim=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Read backend dense table vectors\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        tid2vals = {}\n",
    "        for line in f.readlines():\n",
    "            if len(line) == 0: continue\n",
    "            units = line.split(delim)\n",
    "            table_id, vals = units[0], units[1:] \n",
    "            tid2vals[table_id] = np.array(vals).astype(float)\n",
    "    return tid2vals\n",
    "\n",
    "\n",
    "def trim_col(tname, col):\n",
    "    \"\"\"\n",
    "    Normalize table name.\n",
    "    :tname: table name\n",
    "    :col: column name to be trained\n",
    "    \"\"\"\n",
    "    if tname == '' or col == '': return col\n",
    "    if tname == ' ' or col == ' ': return col\n",
    "    tname_tokens = nlp(tname)\n",
    "    col_tokens = nlp(col)\n",
    "    if tname_tokens[-1].lemma_ == col_tokens[0].lemma_:\n",
    "        return col_tokens[1:]\n",
    "    return col    \n",
    "    \n",
    "# template1 = lambda table_name, col: f\"{table_name[0].capitalize() + table_name[1:]} {trim_col(table_name, col)}.\"\n",
    "\n",
    "def trim_col_naive(tname, col):\n",
    "    if tname == '' or col == '': return col\n",
    "    if tname.lower() == col.lower():\n",
    "        return 'name'\n",
    "    return col.lower()\n",
    "        \n",
    "\n",
    "def trim_type(col_type):\n",
    "    if col_type in [\"text\", \"bool\"]:\n",
    "        return \"\"\n",
    "    return \" \" + \"time\" if col_type == \"date\" else \"number\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501579d",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826d182-4d22-4679-93e5-c1bb30f28fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/tid2tables.pkl\",\"rb\") as f:\n",
    "     tid2tables = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0693d83-fca6-458b-97e7-03896fdbfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tid = {i:tid for i,tid in enumerate(tid2tables.keys())}\n",
    "tid2idx = {tid:i for i,tid in enumerate(tid2tables.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41754303-ca9d-493b-b242-2a73b4ddacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "spiders = []\n",
    "with open(\"./data/spider/spider-tables.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        table = json.loads(line)\n",
    "        spiders.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72870464-6e34-4f69-abb4-563471b24d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtqs = []\n",
    "with open(\"./data/WTQ/wtq-tables.jsonl\", \"r\") as f: # no table name, no domain name\n",
    "    for line in f.readlines():\n",
    "        table = json.loads(line) # dict_keys(['file_name', 'table_name', 'column_types', 'column_names', 'column_values'])\n",
    "        wtqs.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee028e9-be3a-466d-8001-69ec2fa69d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsqls_train = []\n",
    "with open(\"./data/wikisql/wikisql-tables.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        table = json.loads(line) # dict_keys(['refer_cols_index', 'domain', 'table_id', 'table_name', 'column_names', 'column_types', 'column_values'])\n",
    "        wsqls_train.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c1f50-7246-42d3-83c4-438fe106ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/wikisql/wikisql_train.tables2question.json\", \"r\") as f:\n",
    "    wsql_train_table2qs = json.load(f)\n",
    "\n",
    "with open(\"./data/spider/spider-table2questions.json\", \"r\") as f:\n",
    "    spider_table2qs = json.load(f)\n",
    "\n",
    "with open(\"./data/WTQ/wtq-table2questions.json\", \"r\") as f:\n",
    "    wtq_table2qs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73c3d7-4699-4e50-8c2b-ff264032d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word, word2idx = {}, {}\n",
    "word2vec = {}\n",
    "with open(\"./data/numberbatch/nb_emb.txt\", \"r\") as f:\n",
    "    cnt = -2\n",
    "    for line in tqdm(f.readlines(), desc=\"Building word2vec...\", leave=True):\n",
    "        cnt += 1\n",
    "        if cnt == -1: continue\n",
    "        units = line.split(\" \")\n",
    "        word, emb = units[0], np.array(units[1:]).astype(float)\n",
    "        word2vec[word] = emb\n",
    "        idx2word[cnt] = word\n",
    "        word2idx[word] = cnt\n",
    "EMB_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cda5b8-d6f6-4d67-bb9c-8661a8019ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/syndict_pipeline.json\") as f:\n",
    "    synonym_dic = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda2742",
   "metadata": {},
   "source": [
    "# Dense Retrieval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb4c21-3103-47b2-b655-28ea8ba0774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import TapasModel, TapasConfig, TapasTokenizer, BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "def build_projection_layer(weight_path: str):\n",
    "    with open(weight_path, 'rb') as f:\n",
    "        weights = torch.from_numpy(np.load(f))\n",
    "    linear = nn.Linear(weights.size(0), weights.size(1), bias=False)\n",
    "    linear.weight.data = weights\n",
    "    return linear\n",
    "\n",
    "\n",
    "MAX_LEN = 1024\n",
    "DUMMY_TABLE = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88451d0-5656-44ba-aec4-e8ed87f659af",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = os.path.join(\"tapas-torch\", \"tapas_retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf058362-2437-4d2f-b416-aa4f811ae4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_model_path = os.path.join(basepath, \"tapas_nq_hn_retriever_large_table\", \"checkpoint\")\n",
    "table_model = TapasModel.from_pretrained(table_model_path).to(device)\n",
    "tapas_tokenizer = TapasTokenizer.from_pretrained(table_model_path)\n",
    "table_model_config = TapasConfig.from_pretrained(table_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea8dc5-b13d-4a5e-9142-c23d9b696ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model_path = os.path.join(basepath, \"tapas_nq_hn_retriever_large_query\", \"checkpoint\")\n",
    "query_model = TapasModel.from_pretrained(query_model_path).to(device)\n",
    "text_projection_layer = build_projection_layer(os.path.join(basepath, \"projection_layer\", \"text_projection.npy\")).to(device)\n",
    "table_projection_layer = build_projection_layer(os.path.join(basepath, \"projection_layer\", \"table_projection.npy\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1fc5a-8073-4b25-bdc9-dca2c6d5f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_table(dic_table, col_name_key=\"column_names\", max_row_limit=10, max_cell_val_len=50): # output a dataframe\n",
    "    \"\"\"\n",
    "    Build source table text for dense vector computation. Done via resampling strategy.\n",
    "    :dic_table: table as a dictionary\n",
    "    :col_name_key: the key name in passed dic_table storing column names (as a list).\n",
    "    :max_row_limit: maximum number of rows for cosntructed table \n",
    "    :max_cell_val_len: cell values will be truncated to this length.\n",
    "    \"\"\"\n",
    "    col_names = dic_table[\"column_names\"]\n",
    "    col_vals = {k : list(set(v)) for k,v in dic_table[\"column_values\"].items()}\n",
    "    try:\n",
    "        longest_unique = min(max([len(v) for v in col_vals.values()]), max_row_limit)\n",
    "    except:\n",
    "        if len(dic_table[col_name_key]) == 0:\n",
    "            return DUMMY_TABLE\n",
    "        else:\n",
    "            return pd.DataFrame({k:[] for k in dic_table[col_name_key]})\n",
    "    col2vals = {n : [str(elem)[:max_cell_val_len] for elem in np.random.choice(v, longest_unique, replace=True)] for n,v in col_vals.items()}\n",
    "    return pd.DataFrame(col2vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3a861-3027-4d97-9209-fb1ba0cb6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while to load 615144 * 2 vectors...\n",
    "wdc_dense_a = read_dense_table_vectors(path=\"./wdc/wdc_dense_A.txt\")\n",
    "wdc_dense_b = read_dense_table_vectors(path=\"./wdc/wdc_dense_B.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73123c44-48c1-4297-a9b0-cedda27e0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tid = {i:k for i,k in enumerate(wdc_dense_a.keys())}\n",
    "tid2idx = {k:i for i,k in idx2tid.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cb702-f9d7-4f71-b769-7a8391303c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_mat_A = torch.stack([torch.Tensor(vs) for vs in wdc_dense_a.values()], dim = 0).to(device)\n",
    "bm_mat_B = torch.stack([torch.Tensor(vs) for vs in wdc_dense_b.values()], dim = 0).to(device)\n",
    "bm_mat_A = bm_mat_A / torch.norm(bm_mat_A, dim=-1).unsqueeze(-1)\n",
    "bm_mat_B = bm_mat_B / torch.norm(bm_mat_B, dim=-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494bc692",
   "metadata": {},
   "source": [
    "# Core NLI algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88688a8b-8353-4cd8-9f7b-43446c1ec8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_col_name(table_name, col_name):\n",
    "    if table_name == col_name:\n",
    "        return table_name + \" name\"\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eeec8c-3ddc-4e2f-b2dd-11e178090ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spell(col_name):\n",
    "    \"\"\"\n",
    "    Check whether a column name (multiwords allowed) is valid english word.\n",
    "    \"\"\"\n",
    "    return all([checker.check(w) for w in col_name.split(\" \") if w != \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b98630-2023-4278-bc4e-6471f4464e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(pair_dict):\n",
    "    \"\"\"\n",
    "    form batch of a pair of ori-rpl.\n",
    "    Two directions.\n",
    "    \"\"\"\n",
    "    split_idx = []\n",
    "    batch_ori = []\n",
    "    batch_rpl = []\n",
    "    prev_end_idx = 0\n",
    "    for dic in pair_dict:\n",
    "        key_map, pairs, _, _ = dic.values()\n",
    "        split_idx.append((prev_end_idx, prev_end_idx + 2 * len(pairs),))\n",
    "        prev_end_idx = prev_end_idx + 2 * len(pairs) # 2 * because of reverse\n",
    "        for (ori, rpl) in pairs:\n",
    "            batch_ori.append(ori)\n",
    "            batch_ori.append(rpl) # reverse\n",
    "            batch_rpl.append(rpl)\n",
    "            batch_rpl.append(ori) # reverse\n",
    "    return split_idx, batch_ori, batch_rpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0343e-7df8-43cf-a17c-8d4b5ce2b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(split_idx, scores, strict=True):\n",
    "    contras, neus, ents = [],[],[]\n",
    "    for s,e in split_idx:\n",
    "        one_rpl_scores = scores[s:e,:]\n",
    "        if strict:\n",
    "            \"\"\" For REPLACE cols\n",
    "                Prefer high PRECISION of repalceablility! (If we REPLACE with a UNreplaceable col, we run into trouble)\n",
    "                Reject as many LOW confidence candidate as possible.\n",
    "                If NLI give HIGH ent-score, then two columns should almost always be mutally replaceable!\n",
    "            \"\"\"\n",
    "            contra, neu, ent = torch.min(one_rpl_scores, dim=0)[0].squeeze()\n",
    "        else:\n",
    "            \"\"\" For ADD cols\n",
    "               Prefer high RECALL of repalceablility! (If we ADD a replaceable col, we run into trouble)\n",
    "               Accept as many LOW confidence candidate as possible.\n",
    "               If NLI still suggests LOW ent-score, then two columns should almost always be mutally UNreplaceable!\n",
    "            \"\"\"\n",
    "            contra, neu, ent = torch.max(one_rpl_scores, dim=0)[0].squeeze() # Prefer high recall\n",
    "        contras.append(float(contra.item()))\n",
    "        neus.append(float(neu.item()))\n",
    "        ents.append(float(ent.item()))\n",
    "    return contras, neus, ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680132d7-1899-4b20-b23f-cd3b910f9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pairs_for_nli_test(tables, table_id_key=\"table_id\", table_name_key=\"table_name\",\n",
    "                                 col_type_key=\"column_types\", col_name_key=\"column_names\",\n",
    "                                 pending_rpls_key=\"column_names_syn\"):\n",
    "    \"\"\"\n",
    "    Given a list of dictionary-represneted tables, and pending replacements cols,\n",
    "    construct pairs of ori-rpl\n",
    "    :table_id_key: key name in table dict for table name \n",
    "    :col_type_key: key name in table dict for types of columns\n",
    "    :col_name_key: key name in table dict for names of columns\n",
    "    :pending_rpls_key: key name in table dict for pending keys to be replaced\n",
    "    \"\"\"\n",
    "    assert isinstance(tables, list), \"Please pass a list of tables.\"\n",
    "    assert table_id_key in tables[0], \"Each Table must have an id.\"\n",
    "    assert table_name_key in tables[0], \"Table name is required, but the key is missing.\"\n",
    "    assert col_name_key in tables[0], \"column name key is required but missing\"\n",
    "    assert col_type_key in tables[0], \"column type key is required but missing\"\n",
    "    assert pending_rpls_key in tables[0], \"Pending replacement columns is required, but the key is missing.\"\n",
    "    constructed_pairs = []\n",
    "    for i in trange(len(tables)):\n",
    "        tab = tables[i]\n",
    "        tname = tab[table_name_key] if tab[table_name_key] != \"s\" else tab[table_name_key][:-1]\n",
    "        pending_rpls = tab[pending_rpls_key]\n",
    "        col2type = {col: tp for col,tp in zip(tab[col_name_key], tab[col_type_key])}\n",
    "        for ori_col, rpl_col_list in pending_rpls.items():\n",
    "            for rpl_col in rpl_col_list:\n",
    "                if not check_spell(rpl_col): continue\n",
    "                rpl_dic = {\"key_map\": None, \"pairs\": [], \"table_id\": tab[table_id_key], \"table_name\": tab[table_name_key]}\n",
    "                for template in TEMPLATES:\n",
    "                    sent_ori = template(tname, ori_col, col2type[ori_col])\n",
    "                    sent_rpl = template(tname, rpl_col, col2type[ori_col])\n",
    "                    rpl_dic[\"key_map\"] = (ori_col, rpl_col,)\n",
    "                    rpl_dic[\"pairs\"] += ((sent_ori, sent_rpl,),)\n",
    "                constructed_pairs.append(rpl_dic)\n",
    "    return constructed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c673035-2917-46c6-8cf0-6ef3faabe3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nli_test_across_tables(constructed_pairs, batch_size=256):\n",
    "    \"\"\"\n",
    "    The major interface for NLI verification.\n",
    "    Given constructed pairs (results from construct_pairs_for_nli_test),\n",
    "    use batch computation to speed up the verfication process.\n",
    "    \"\"\"\n",
    "    assert batch_size % 8 == 0, \"Batch size must be a multiple of 8.\"\n",
    "    results = []\n",
    "    completed_pairs = 0\n",
    "    total_batches = len(constructed_pairs) // batch_size + 1\n",
    "    pbar = tqdm(total = total_batches)\n",
    "    with torch.no_grad():\n",
    "        while completed_pairs < len(constructed_pairs):\n",
    "            batch_contras, batch_neus, batch_ents = [],[],[]\n",
    "            prev_completed = completed_pairs\n",
    "            completed_pairs = min(completed_pairs + batch_size, len(constructed_pairs))\n",
    "            batch = constructed_pairs[prev_completed:completed_pairs]\n",
    "            split_idx, batch_ori, batch_rpl = batchify(batch)\n",
    "            inputs = nli_tokenizer(batch_ori, batch_rpl, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "            logits = nli_model(**inputs).logits\n",
    "            scores = softmax(logits, dim=1) # [batch, 3]\n",
    "            batch_contras, batch_neus, batch_ents = aggregate(split_idx, scores)\n",
    "            del inputs; del logits; del scores; torch.cuda.empty_cache()\n",
    "            batch_contras, batch_neus, batch_ents = np.array(batch_contras), np.array(batch_neus), np.array(batch_ents)\n",
    "            for b, c, n, e in zip(batch, batch_contras, batch_neus, batch_ents):            \n",
    "                results.append({\"key_map\": b[\"key_map\"], \"scores\": (c, n, e,)})\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8a541-ec86-4bf1-a60e-168a246b88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_name(text):\n",
    "    for ch in ['\\\\','`','*','{','}','[',']','(',')','>', '<', '#','+','\\'', '\"']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, \"\")\n",
    "    text.replace(\"-\", \" \")\n",
    "    text.replace(\".\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2879d-ed52-4f7f-b71b-5bce0d99fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emb(list_of_names):\n",
    "    \"\"\"\n",
    "    Extract numberbatch word embeddings for a given list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(list_of_names, list), \"Expected list as input\"\n",
    "    output_matrix = np.zeros([len(list_of_names), EMB_DIM])\n",
    "    for i,name in enumerate(list_of_names):\n",
    "        name = trim_name(name)\n",
    "        units = name.split() # notice \"_\" is covered by our nb_emb!\n",
    "        name_emb = np.zeros(EMB_DIM)\n",
    "        for word in units:\n",
    "            if \"_\" in word and word2vec.get(word, None) is None:\n",
    "                sub_words = word.split(\"_\")\n",
    "                local_emb_mat = extract_emb(sub_words)\n",
    "                emb = np.mean(local_emb_mat, axis=0)\n",
    "            else:\n",
    "                emb = word2vec.get(word, np.zeros(EMB_DIM))\n",
    "            name_emb += emb\n",
    "        name_emb /= len(units)\n",
    "        output_matrix[i,:] = name_emb\n",
    "    return output_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c51f8-302d-43cb-9c62-6aa71cbeee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker(tgt_names, cand_names, topk=10):\n",
    "    \"\"\"\n",
    "    Do reranking (usually among few hundreds of candidates) and return topk per numberbatch word2vec similarity\n",
    "    \"\"\"\n",
    "    if len(cand_names) == 0:\n",
    "        return {}\n",
    "    tgt_mat = extract_emb(tgt_names)\n",
    "    cand_mat = extract_emb(cand_names)\n",
    "    sim_mat = tgt_mat @ cand_mat.T\n",
    "    topk = min(len(cand_names), topk)\n",
    "    top_scores, top_idx = [v.squeeze().numpy() for v in torch.topk(torch.Tensor(sim_mat), topk, dim=-1)]\n",
    "    rec_dic = {}\n",
    "    for i, tgt in enumerate(tgt_names):\n",
    "        if len(top_idx.shape) == 0: top_idx = np.array([top_idx])\n",
    "        if len(top_idx.shape) == 1: top_idx = top_idx[None, :]\n",
    "        \n",
    "        rec_dic[tgt] = [cand_names[idx] for idx in top_idx[i]]\n",
    "    return rec_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a897d9d-3a69-4dbe-809e-be7d3346db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriver(query_table, queries=None, retrieve_strategy=\"query_dense\", topk_tables=50, col_name_key=\"column_names\", target_expand_keys=100):\n",
    "    \"\"\"\n",
    "    Tapas based dense retrieval for finding topk most similar tabels from table base.\n",
    "    :query_table: The table whose topk similar will be found\n",
    "    :quries: the user NL queries attached with the query_table\n",
    "    :retrieve_strategy: Choose from [\"query_dense\", \"table_dense\"], qd uses NL query as retrieval query vector,\n",
    "                        and td uses table as retreival query vector.\n",
    "    :topk_tables: Return k most similar tables\n",
    "    \"\"\"\n",
    "    ori_cols = set(query_table[col_name_key])\n",
    "    top_tables_tid = []\n",
    "    if retrieve_strategy == \"query_dense\":\n",
    "        top_tables_tid = retrieve_tables_query_dense(queries,k=topk_tables)\n",
    "    elif retrieve_strategy == \"table_dense\":\n",
    "        pass\n",
    "    elif retrieve_strategy == \"tfidf\":\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    top_tables = [tid2tables[tid] for tid in top_tables_tid]\n",
    "    expanded_cols = set()\n",
    "    for t in top_tables:\n",
    "        if len(expanded_cols) >= target_expand_keys: break\n",
    "        expanded_cols = expanded_cols.union(t[col_name_key])\n",
    "    expanded_cols = expanded_cols.difference(ori_cols)\n",
    "    return list(ori_cols), list(expanded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a9270-9d7f-4f46-ae83-f6fc10412284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tables_tfidf(query_table, tfidf_mat, col_name_key=\"column_names\", table_doc_key=\"doc\"):\n",
    "    \"\"\"\n",
    "    TF-IDF based retrieval for finding most similar tables from DB.\n",
    "    \"\"\"\n",
    "    assert table_doc_key in query_table and col_name_key in query_table\n",
    "    query_tfidf = vectorizer.transform(query_table)\n",
    "    scores = cosine_similarity(query_tfidf, tfidf_mat)[0]\n",
    "    top_scores, indices = [t.squeeze().numpy() for t in torch.topk(torch.Tensor(scores), 1000)]\n",
    "    return [idx2tid[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030f2df-e6c4-4b48-902d-7c904caf8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tables_query_dense(queries, k=50):\n",
    "    \"\"\"'\n",
    "    Interface for finding most similar table via dense retrieval. call goes from here.\n",
    "    \"\"\"\n",
    "    assert isinstance(queries, list), \"input queries must be a list of strings\"\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        q_inputs = tapas_tokenizer(table=DUMMY_TABLE, queries=queries, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        qb = query_model(**q_inputs).pooler_output\n",
    "        qb = text_projection_layer(qb)\n",
    "        qb = qb / torch.norm(qb, dim=-1).unsqueeze(-1)\n",
    "        cos = torch.matmul(qb, bm_mat_B.transpose(0, 1))\n",
    "        cos = torch.mean(cos, dim=0)\n",
    "        top_score, top_idx = [v.data.cpu().numpy() for v in torch.topk(cos, k=k)]\n",
    "        \n",
    "#         top_idx = top_idx.data.cpu().numpy()\n",
    "\n",
    "    #### ab means  table encoded with encoder A, query encoded with encoder B.\n",
    "    return  [idx2tid[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae38db6-3281-4868-af89-4d2656da1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = enchant.Dict(\"en_US\")\n",
    "def _ends_with_id(string):\n",
    "    if len(string) < 2: return False\n",
    "    return string[-2:].lower() == \"id\"\n",
    "\n",
    "def _fill_type_info(string, col_type, delim):\n",
    "    \"\"\"\n",
    "    Add type description for a given column\n",
    "    \"\"\"\n",
    "    if col_type == \"date\" and not any([dm in string for dm in date_marks]):\n",
    "        return delim.join([string, \"time\"])\n",
    "    if col_type == \"number\" and not any([nm in string for nm in num_marks]):\n",
    "        return delim.join([string, \"number\"])\n",
    "    return string\n",
    "\n",
    "\n",
    "def contains_number(text):\n",
    "    \"\"\"\n",
    "    Judege whether the passed string contain number\n",
    "    \"\"\"\n",
    "    return len(re.findall(\"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", text)) > 0\n",
    "\n",
    "\n",
    "def _get_replacement(tok1, tok2, tok1_is_reserved, tok2_is_reserved):\n",
    "    \"\"\"\n",
    "    Given a bi-gram, replce the word whose IDF is higher with its synonym.\n",
    "    \"\"\"\n",
    "    if tok1_is_reserved and tok2_is_reserved:\n",
    "        return (None, None)\n",
    "    if tok1_is_reserved and (not check_spell(tok2) or contains_number(tok2)):\n",
    "        return (None, None)\n",
    "    if tok2_is_reserved and (not check_spell(tok1) or contains_number(tok1)):\n",
    "        return (None, None)\n",
    "    if tok1_is_reserved:\n",
    "        syn_dic = synonym_dic.get(tok2.lower(), None)\n",
    "        return (tok2, syn_dic) if syn_dic is not None else (None, None)\n",
    "    if tok2_is_reserved:\n",
    "        syn_dic = synonym_dic.get(tok1.lower(), None)\n",
    "        return (tok1, syn_dic) if syn_dic is not None else (None, None)\n",
    "    # both are not reserved, pick one with higher tfidf val\n",
    "    def extract_idf(vocab):\n",
    "        vocab_idx = vectorizer.vocabulary_.get(vocab, None)\n",
    "        idf = 0 if vocab_idx is None else vectorizer.idf_[vocab_idx]\n",
    "        return idf\n",
    "    first_tgt = tok1 if extract_idf(tok1) <= extract_idf(tok2) else tok2  # rare is better\n",
    "    second_tgt = tok2 if first_tgt == tok1 else tok1\n",
    "    syn_dic_first = synonym_dic.get(first_tgt.lower(), None)\n",
    "    if syn_dic_first is not None: return (first_tgt, syn_dic_first)\n",
    "    syn_dic_second = synonym_dic.get(second_tgt.lower(), None)\n",
    "    return (second_tgt, syn_dic_second) if syn_dic_second is not None else (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17563d69",
   "metadata": {},
   "source": [
    "# REPLACE & ADD Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7400d7f-e100-473c-9f54-916aef6ca2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_token(token):\n",
    "    \"\"\"\n",
    "    Do strict noramlization for a given token. All punctuations will be removed.\n",
    "    \"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(token))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a3874-6f29-4397-80a9-b54423edf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_retrieval_results(replacement_dict):\n",
    "    \"\"\"\n",
    "    All overly short (char len < 4) / misspelled / contains numbers tokens are not replaceable. Filter them out.\n",
    "    \"\"\"\n",
    "    out_dic = deepcopy(replacement_dict)\n",
    "    for col in replacement_dict.keys():\n",
    "        replacements = replacement_dict[col]\n",
    "        filter_rpls = [r for r in replacements if len(r) > 4]\n",
    "        out_dic[col] = filter_rpls\n",
    "    return out_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ef1ad-e030-4e5e-969e-ed6e8ed096d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consider_rpl(token):\n",
    "    \"\"\"\n",
    "    Judge whether a token is suitable for replacement.\n",
    "    All overly short (char len < 4) / misspelled / contains numbers col are not replaceable.\n",
    "    \"\"\"\n",
    "    norm_token = normalize_token(token)\n",
    "    if len(norm_token) < 4: return False, token\n",
    "    if contains_number(norm_token): return False, token\n",
    "    if not check_spell(norm_token): return False, token\n",
    "    return True, norm_token\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1413b37-c9f2-4730-a78d-354f3286f236",
   "metadata": {},
   "source": [
    "## core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587376d-ebf0-4a41-9106-e6f5ba9c1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_and_add_for_give_tables(path, table2qs, batch_size=512,\n",
    "                                    replace_threshold=0.75, \n",
    "                                    add_threshold=0.4,\n",
    "                                    output_prefix=\"\", output_dir=\"./processed_data\",\n",
    "                                    delim=\" \",\n",
    "                                    topk_tables=50,\n",
    "                                    max_cands_per_col=10,\n",
    "                                    table_name_key=\"table_name\",\n",
    "                                    col_type_key=\"column_types\",\n",
    "                                    col_name_key=\"column_names\"):\n",
    "    \"\"\"\n",
    "    The highest-level interface for replacement and addition across all tables stored in a given path.\n",
    "    One call, handle all.\n",
    "    :path: Target tables path\n",
    "    :table2qs: The queries corresponding to the each of the tables.\n",
    "    :batch_size: bsz for NLI checking. 512 recommended.\n",
    "    :replace_threshold: If NLI entailment score is higher than this threshold under STRICT mode, then the rpl pair is accepted.\n",
    "    :add_treshold: If NLI entailment score is lower than this threshold under LOOSE mode, then the add pair is accepted.\n",
    "    :output_prefix: File name prefix for output file.\n",
    "    :output_dir: Output file directory.\n",
    "    :delim: Delimiator for column names. Single white space by default.\n",
    "    :topk_tables: How many most similar tables to consider from dense retrieval.\n",
    "    :max_cands_per_col: Max nubmer of pairs to be considered for each column (both add and rpl).\n",
    "                        This directly influences the final amount to be checked by NLI.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # STEP 0: Prepare tables\n",
    "    print(\"STEP 0 : Prepare tables...\\n\")\n",
    "    tables = []\n",
    "    tables_template = {}\n",
    "    cnt = 0\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            table = json.loads(line)\n",
    "            table_copy = deepcopy(table)\n",
    "            table_copy[\"rpls_retrieval\"] = {}\n",
    "            table_copy[\"rpls_syndict\"] = {}\n",
    "            tables.append(table_copy)\n",
    "            table[\"REPLACE\"] = {tname:[] for tname in table[col_name_key]}\n",
    "            table[\"ADD\"] = {tname:[] for tname in table[col_name_key]}\n",
    "            tables_template[table[\"table_id\"]] = table\n",
    "#             if cnt == 5: break\n",
    "            cnt += 1\n",
    "            \n",
    "    # STEP 1: \"retrieval\"  for add / replacement\n",
    "    print('STEP 1: Dense retrieval for add / replacement...\\n')\n",
    "    for tab in tqdm(tables, position=0):\n",
    "        tid = tab[\"table_id\"]\n",
    "        queries = table2qs.get(tid, None)\n",
    "        if queries is not None:\n",
    "            queries = queries[:10] if len(queries) > 10 else queries\n",
    "            ori_cols, expanded_cols = retriver(query_table=tab, queries=queries, topk_tables=topk_tables)\n",
    "            rec_dic = reranker(ori_cols, expanded_cols, topk=max_cands_per_col) # We will find analog & synonyms in this list\n",
    "            tab[\"rpls_retrieval\"] = trim_retrieval_results(rec_dic)\n",
    "            \n",
    "                \n",
    "    # STEP 2 : synonym dict for replacement\n",
    "    print('STEP 2 : synonym dict for replacement...\\n')\n",
    "    for tab in tqdm(tables, position=1):\n",
    "        if isinstance(tab[\"column_types\"], list):\n",
    "            col2type = {c:t for c,t in zip(tab[\"column_names\"], tab[\"column_types\"])}\n",
    "        else:\n",
    "            col2type = tab[\"column_types\"]\n",
    "        for col in tab[\"column_names\"]:\n",
    "            col_type = col2type.get(col, \"text\")\n",
    "            if _ends_with_id(col): continue\n",
    "            tokens = [w.lower() for w in col.split(delim)]\n",
    "            keep_original, normalized_tokens = [], []\n",
    "            tok2syn = {}\n",
    "            for tok in tokens:\n",
    "                can_rpl, tok = consider_rpl(tok)\n",
    "                normalized_tokens.append(tok)\n",
    "                syn_dic = synonym_dic.get(tok, None)\n",
    "                if syn_dic is None: syn_dic = synonym_dic.get(get_singular_word(tok), None)\n",
    "                keep_ori = (not can_rpl) or (syn_dic is None)  # skip eihter because not replaceable or not in dic\n",
    "                keep_original.append(keep_ori)\n",
    "                if keep_ori == True: continue\n",
    "                rec_dic = reranker(tgt_names=[tok], cand_names=list(set(syn_dic[\"synonyms\"])), topk=10)\n",
    "                tok2syn.update(rec_dic)\n",
    "            syn_rpl_candidates = set() # Genereate syn-replaced candidates\n",
    "            patience = 5 # if 5 in steps there is no new candidate added, break the loop.\n",
    "            \n",
    "            while True:\n",
    "                if len(syn_rpl_candidates) >= max_cands_per_col or patience == 0:\n",
    "                    syn_rpl_candidates = list(syn_rpl_candidates.difference(set([\" \".join(normalized_tokens)])))\n",
    "                    break\n",
    "                rpl_threshold = 1 if len(tokens) == 1 else (0.75 if len(tokens) == 2 else 0.5)\n",
    "                do_rpl_coins = np.random.rand(len(tokens)) <= rpl_threshold  # only keep original for 20% of time\n",
    "                new_cand = []\n",
    "                for i, tok in enumerate(normalized_tokens):\n",
    "                    if not keep_original[i] and do_rpl_coins[i]:\n",
    "                        all_syns = tok2syn.get(tok, [tok])\n",
    "                        syn = np.random.choice(all_syns)\n",
    "                        new_cand.append(syn)\n",
    "                    else:\n",
    "                        new_cand.append(tok)\n",
    "                new_cand = delim.join(new_cand)\n",
    "                len_before = len(syn_rpl_candidates)\n",
    "                syn_rpl_candidates.add(new_cand)\n",
    "                if len(syn_rpl_candidates) > len_before:\n",
    "                    patience = 5\n",
    "                else:\n",
    "                    patience -= 1\n",
    "            tab[\"rpls_syndict\"].update({col : syn_rpl_candidates})\n",
    "    \n",
    "    \n",
    "    # #STEP 3: filter syn dict replacement with NLI\n",
    "    print('STEP 3: filter syn dict replacement with NLI...\\n')\n",
    "    STRICT_MODE = True\n",
    "    constructed_pairs_rpl_syndict = construct_pairs_for_nli_test(tables, pending_rpls_key=\"rpls_syndict\",\n",
    "                                                     table_name_key=table_name_key,\n",
    "                                                     col_type_key=col_type_key,\n",
    "                                                     col_name_key=col_name_key)\n",
    "    results_rpl_syndict = nli_test_across_tables(constructed_pairs_rpl_syndict, batch_size=batch_size)\n",
    "    for i, dic in enumerate(results_rpl_syndict):\n",
    "        table_id = constructed_pairs_rpl_syndict[i][\"table_id\"]\n",
    "        table = tables_template[table_id]\n",
    "        ent = dic[\"scores\"][2]\n",
    "        if ent >= replace_threshold:\n",
    "            ori, rpl = dic[\"key_map\"]\n",
    "#             print(f\"{ori} -> {rpl}\")\n",
    "            table[\"REPLACE\"][ori].append(rpl)  # update REPLACE key\n",
    "#     print(results_rpl_syndict)\n",
    "    \n",
    "    \n",
    "    # STEP 4: filter retrieval replacement with NLI\n",
    "    print('STEP 4: filter retrieval replacement with NLI...\\n')\n",
    "    STRICT_MODE = True\n",
    "    constructed_pairs_rpl_retrieval = construct_pairs_for_nli_test(tables, pending_rpls_key=\"rpls_retrieval\",\n",
    "                                                     table_name_key=table_name_key,\n",
    "                                                     col_type_key=col_type_key,\n",
    "                                                     col_name_key=col_name_key)\n",
    "    results_rpl_retrieval = nli_test_across_tables(constructed_pairs_rpl_retrieval, batch_size=batch_size)\n",
    "    for i, dic in enumerate(results_rpl_retrieval):\n",
    "        table_id = constructed_pairs_rpl_retrieval[i][\"table_id\"]\n",
    "        table = tables_template[table_id]\n",
    "        ent = dic[\"scores\"][2]\n",
    "        if ent >= replace_threshold:\n",
    "            ori, rpl = dic[\"key_map\"]\n",
    "            table[\"REPLACE\"][ori].append(rpl)  # update REPLACE key\n",
    "\n",
    "    \n",
    "    \n",
    "    # STEP 5: prune all replaceable from retrieval results & filter substring overlap from original col\n",
    "    print('STEP 5: prune all replaceable from retrieval results & filter substring overlap from original col...\\n')\n",
    "    for table in tables:\n",
    "        tid = table[\"table_id\"]\n",
    "        all_columns = table[col_name_key]\n",
    "        rpl_dict = tables_template[tid][\"REPLACE\"]\n",
    "        for rpl_col in table[\"rpls_retrieval\"].keys():\n",
    "            rpl_candidates = table[\"rpls_retrieval\"][rpl_col]\n",
    "            add_candidates = [] # ADD operation candidates comes from here\n",
    "            for rpl in rpl_candidates:\n",
    "                if any([rpl in c for c in all_columns]) or any([rpl in c for c in rpl_dict[rpl_col]]):\n",
    "                    continue\n",
    "                add_candidates.append(rpl)\n",
    "            table[\"rpls_retrieval\"][rpl_col] = add_candidates\n",
    "#     print(tables[8][\"rpls_retrieval\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # STEP 6 :  filter leftover retrieval ADD candidates with NLI\n",
    "    print('STEP 6 :  filter leftover retrieval ADD candidates with NLI\\n')\n",
    "    STRICT_MODE = False\n",
    "    constructed_pairs_add_retrieval = construct_pairs_for_nli_test(tables, pending_rpls_key=\"rpls_retrieval\",\n",
    "                                                     table_name_key=table_name_key,\n",
    "                                                     col_type_key=col_type_key,\n",
    "                                                     col_name_key=col_name_key)\n",
    "    results_add_retrieval = nli_test_across_tables(constructed_pairs_add_retrieval, batch_size=batch_size)\n",
    "    for i, dic in enumerate(results_add_retrieval):\n",
    "        table_id = constructed_pairs_add_retrieval[i][\"table_id\"]\n",
    "        table = tables_template[table_id]\n",
    "        ent = dic[\"scores\"][2]\n",
    "        if ent <= add_threshold:\n",
    "            ori, rpl = dic[\"key_map\"]\n",
    "            table[\"ADD\"][ori].append(rpl)  # update ADD key\n",
    "#     print(tables_template[\"SPIDER_8\"][\"ADD\"])\n",
    "    \n",
    "    # STEP 7: Write replace + add results to new file\n",
    "    print('STEP 7: Write REPLACE & ADD results to new file \\n')\n",
    "    with open(f\"{output_dir}/{output_prefix}-pipeline-output.jsonl\", \"w\") as f:\n",
    "        for table in tables_template.values():\n",
    "            json.dump(table, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335f512",
   "metadata": {},
   "source": [
    "# Leave For Running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb93a09-2463-477d-82f5-18cb8a4438ba",
   "metadata": {},
   "source": [
    "## Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661305e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_and_add_for_give_tables(\"./data/spider/spider-tables.jsonl\",\n",
    "                         table2qs=spider_table2qs,\n",
    "                         output_prefix=\"spider\",\n",
    "                         table_name_key=\"table_name\",\n",
    "                         replace_threshold=0.50,\n",
    "                         batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfe33c-8f88-4057-9aee-07e65d52096a",
   "metadata": {},
   "source": [
    "## WTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_and_add_for_give_tables(\"./data/WTQ/wtq-tables.jsonl\",\n",
    "                         table2qs=wtq_table2qs,\n",
    "                         output_prefix=\"wtq\",\n",
    "                         table_name_key=\"pred_table_name\",\n",
    "                         replace_threshold=0.50,\n",
    "                         batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9c483-949f-4827-9f8f-bd0635ec041d",
   "metadata": {},
   "source": [
    "## WikiSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6db3ec-1123-4952-9797-ef33d48b9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_and_add_for_give_tables(\"./data/wikisql/wikisql-tables.jsonl\",\n",
    "                         table2qs=wsql_train_table2qs,\n",
    "                         output_prefix=\"wsql-train\",\n",
    "                         table_name_key=\"pred_table_name\",\n",
    "                         replace_threshold=0.70,\n",
    "                         batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adveta3.8",
   "language": "python",
   "name": "adveta3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
